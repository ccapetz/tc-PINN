{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor-Compressed PINN for Solving Burgers Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pcdcm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from scipy.stats import qmc\n",
    "import tensorlearn as tl\n",
    "import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##nn basics\n",
    "\n",
    "def read_network(network_path):\n",
    "\n",
    "    saved_model = tf.keras.models.load_model(network_path)\n",
    "\n",
    "    return saved_model\n",
    "\n",
    "\n",
    "\n",
    "def layer_param(nn_model, layer_name):\n",
    "\n",
    "    return nn_model.get_layer(layer_name).weights\n",
    "\n",
    "\n",
    "\n",
    "def get_dense_layer_weights(param):\n",
    "    return param[0].numpy()\n",
    "\n",
    "def get_dense_layer_bias(param):\n",
    "    return param[1].numpy()\n",
    "\n",
    "def set_dense_layer_param_list(weights,bias):\n",
    "    return [weights,bias]\n",
    "\n",
    "\n",
    "    \n",
    "def freez_layer(nn_model,index, param_layer):\n",
    "    nn_model.layers[index].set_weights(param_layer)\n",
    "    nn_model.layers[index].trainable=False\n",
    "    return nn_model\n",
    "\n",
    "\n",
    "def net_retrain(nn_model, ds_train, ds_test, epochs, output_path):\n",
    "\n",
    "\n",
    "    checkpoint_path=output_path\n",
    "    cp_callback = keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 monitor='val_accuracy',\n",
    "                                                 save_weights_only=False,\n",
    "                                                 save_best_only=True,\n",
    "                                                 verbose=1)\n",
    "    nn_model.fit(\n",
    "                ds_train,\n",
    "                epochs=epochs,\n",
    "                validation_data=ds_test,\n",
    "                callbacks=[cp_callback]\n",
    "            )                \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    nn_model.save(output_path)\n",
    "    return nn_model\n",
    "\n",
    "\n",
    "def get_layer_index_by_name(model, layer_name):\n",
    "    for index, layer in enumerate(model.layers):\n",
    "        if layer.name == layer_name:\n",
    "            return index\n",
    "        \n",
    "\n",
    "def inference(nn_model, ds_test):\n",
    "    return nn_model.evaluate(ds_test)\n",
    "\n",
    "\n",
    "\n",
    "def mnist_data_load(batch_size=128):\n",
    "        tfds.disable_progress_bar()\n",
    "        #tf.enable_v2_behavior()\n",
    "        \n",
    "        (ds_train, ds_test), ds_info = tfds.load(\n",
    "            'fashion_mnist',\n",
    "            split=['train', 'test'],\n",
    "            shuffle_files=True,\n",
    "            as_supervised=True,\n",
    "            with_info=True,\n",
    "        )\n",
    "        \n",
    "        def normalize_img(image, label):\n",
    "          \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "          return tf.cast(image, tf.float32) / 255., label\n",
    "        \n",
    "        ds_train = ds_train.map(\n",
    "            normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "        ds_train = ds_train.cache()\n",
    "        ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "        ds_train = ds_train.batch(batch_size)\n",
    "        ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "        \n",
    "        ds_test = ds_test.map(\n",
    "            normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "        ds_test = ds_test.batch(batch_size)\n",
    "        ds_test = ds_test.cache()\n",
    "        ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "        return ds_train, ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "#load model\n",
    "tcPINN = tf.keras.models.load_model(\"burgers.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available.\n"
     ]
    }
   ],
   "source": [
    "#FIX THIS - GPU setup\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) == 0:\n",
    "    print(\"No GPU available.\")\n",
    "else:\n",
    "    print(\"GPU(s) available:\", len(physical_devices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decomposing and recomposing a layer of weights\n",
    "def decomp_recomp(weights, epsilon, shape, layer):\n",
    "\n",
    "    og_shape = np.array(weights.shape)\n",
    "    #print(type(og_shape))\n",
    "    np_weights = np.array(weights)\n",
    "    print(np_weights.shape)\n",
    "    #print(type(np_weights))\n",
    "\n",
    "    weights_3d = np.reshape(np_weights, shape).astype(np.float64) #shape search???\n",
    "\n",
    "    #print(type(weights_3d))\n",
    "\n",
    "    #decompose/recompose\n",
    "    tt_weights = tl.auto_rank_tt(weights_3d, epsilon) #error of 0.5 - 1, tweak w shape search?\n",
    "    recomp_weights = tl.tt_to_tensor(tt_weights)\n",
    "\n",
    "    #data saving\n",
    "    compression_ratio = tl.tt_compression_ratio(tt_weights)\n",
    "    data_saving = 1 - (1/compression_ratio)\n",
    "    print(\"data_saving (%)\", data_saving*100)\n",
    "\n",
    "    #save compressed layer\n",
    "    tt_weights_npy = np.array(tt_weights)\n",
    "    np.save(f'{layer}.npy', tt_weights_npy)\n",
    "\n",
    "    return np.reshape(recomp_weights, og_shape) #og shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate loss\n",
    "\n",
    "### generating data\n",
    "\n",
    "# number of boundary and initial data points\n",
    "# value `Nd` in the reference paper:\n",
    "# Nd = number_of_ic_points + number_of_bc1_points + number_of_bc1_points \n",
    "number_of_ic_points = 50\n",
    "number_of_bc1_points = 25\n",
    "number_of_bc2_points = 25\n",
    "\n",
    "# Latin Hypercube Sampling (LHS) engine ; to sample random points in domain,\n",
    "# boundary and initial boundary\n",
    "engine = qmc.LatinHypercube(d=1)\n",
    "\n",
    "# temporal data points\n",
    "t_d = engine.random(n=number_of_bc1_points + number_of_bc2_points)\n",
    "temp = np.zeros([number_of_ic_points, 1]) # for IC ; t = 0\n",
    "t_d = np.append(temp, t_d, axis=0)\n",
    "# spatial data points\n",
    "x_d = engine.random(n=number_of_ic_points)\n",
    "x_d = 2 * (x_d - 0.5)\n",
    "temp1 = -1 * np.ones([number_of_bc1_points, 1]) # for BC1 ; x = -1\n",
    "temp2 = +1 * np.ones([number_of_bc2_points, 1]) # for BC2 ; x = +1\n",
    "x_d = np.append(x_d, temp1, axis=0)\n",
    "x_d = np.append(x_d, temp2, axis=0)\n",
    "\n",
    "# output values for data points (boundary and initial)\n",
    "y_d = np.zeros(x_d.shape)\n",
    "\n",
    "# for initial condition: IC = -sin(pi*x)\n",
    "y_d[ : number_of_ic_points] = -np.sin(np.pi * x_d[:number_of_ic_points])\n",
    "\n",
    "# all boundary conditions are set to zero\n",
    "y_d[number_of_ic_points : number_of_bc1_points + number_of_ic_points] = 0\n",
    "y_d[number_of_bc1_points + number_of_ic_points : number_of_bc1_points + number_of_ic_points + number_of_bc2_points] = 0\n",
    "\n",
    "# number of collocation points\n",
    "Nc = 10000\n",
    "\n",
    "# LHS for collocation points\n",
    "engine = qmc.LatinHypercube(d=2)\n",
    "data = engine.random(n=Nc)\n",
    "# set x values between -1. and +1.\n",
    "data[:, 1] = 2*(data[:, 1]-0.5)\n",
    "\n",
    "# change names\n",
    "t_c = np.expand_dims(data[:, 0], axis=1)\n",
    "x_c = np.expand_dims(data[:, 1], axis=1)\n",
    "\n",
    "# MSE loss function\n",
    "# IMPORTANT: this loss function is used for data points\n",
    "@tf.function\n",
    "def mse(y, y_):\n",
    "    return tf.reduce_mean(tf.square(y-y_))\n",
    "\n",
    "def calc_loss(model):\n",
    "\n",
    "    # u(t, x) just makes working with model easier and the whole code looks more\n",
    "    # like its mathematical backend\n",
    "    @tf.function\n",
    "    def u(t, x, model):\n",
    "        # model input shape is (2,) and `u` recieves 2 arguments with shape (1,)\n",
    "        # to be able to feed those 2 args (t, x) to the model, a shape (2,) matrix\n",
    "        # is build by simply concatenation of (t, x)\n",
    "        u = model(tf.concat([t, x], axis=1)) # note the axis ; `column`\n",
    "        return u\n",
    "    \n",
    "    # the physics informed loss function\n",
    "    # IMPORTANT: this loss function is used for collocation points\n",
    "    @tf.function\n",
    "    def f(t, x, model):\n",
    "        u0 = u(t, x, model)\n",
    "        u_t = tf.gradients(u0, t)[0]\n",
    "        u_x = tf.gradients(u0, x)[0]\n",
    "        u_xx = tf.gradients(u_x, x)[0]\n",
    "        F = u_t + u0*u_x - (0.01/np.pi)*u_xx\n",
    "        return tf.reduce_mean(tf.square(F))\n",
    "    \n",
    "    # model output/prediction\n",
    "    y_ = u(t_d, x_d, model)\n",
    "    # physics-informed loss for collocation points\n",
    "    L1 = f(t_c, x_c, model)\n",
    "    # MSE loss for data points\n",
    "    L2 = mse(y_d, y_)\n",
    "    loss = L1 + L2\n",
    "    return loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrain model\n",
    "\n",
    "def retrain(model, epochs):\n",
    "\n",
    "    #loss funciton definitions\n",
    "    # u(t, x) just makes working with model easier and the whole code looks more\n",
    "    # like its mathematical backend\n",
    "    @tf.function\n",
    "    def u(t, x, model):\n",
    "        # model input shape is (2,) and `u` recieves 2 arguments with shape (1,)\n",
    "        # to be able to feed those 2 args (t, x) to the model, a shape (2,) matrix\n",
    "        # is build by simply concatenation of (t, x)\n",
    "        u = model(tf.concat([t, x], axis=1)) # note the axis ; `column`\n",
    "        return u\n",
    "    \n",
    "    # the physics informed loss function\n",
    "    # IMPORTANT: this loss function is used for collocation points\n",
    "    @tf.function\n",
    "    def f(t, x, model):\n",
    "        u0 = u(t, x, model)\n",
    "        u_t = tf.gradients(u0, t)[0]\n",
    "        u_x = tf.gradients(u0, x)[0]\n",
    "        u_xx = tf.gradients(u_x, x)[0]\n",
    "        F = u_t + u0*u_x - (0.01/np.pi)*u_xx\n",
    "        return tf.reduce_mean(tf.square(F))\n",
    "\n",
    "    loss_list = []\n",
    "\n",
    "    # L-BFGS optimizer was used in the reference paper\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=5e-4)\n",
    "    start = time.time()\n",
    "\n",
    "    # training loop\n",
    "    # IMPORTANT: a while-based training loop is more beneficial\n",
    "    # updates the model while loss > 0.006\n",
    "    for epoch in range(epochs + 1):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # model output/prediction\n",
    "            y_ = u(t_d, x_d, model)\n",
    "            # physics-informed loss for collocation points\n",
    "            L1 = f(t_c, x_c, model)\n",
    "            # MSE loss for data points\n",
    "            L2 = mse(y_d, y_)\n",
    "            loss = L1 + L2\n",
    "        # compute gradients\n",
    "        g = tape.gradient(loss, model.trainable_weights)\n",
    "        loss_list.append(loss)\n",
    "        # log every 10 epochs\n",
    "        if (not epoch%50) or (epoch == epochs-1):\n",
    "            print(f\"{epoch:4} {loss.numpy()}\")\n",
    "        # apply gradients\n",
    "        opt.apply_gradients(zip(g, model.trainable_weights))\n",
    "\n",
    "    end = time.time()\n",
    "    print(f\"{end - start:.10} (s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "----------------- input layer -----------------\n",
      "(2, 20)\n",
      "data_saving (%) 25.0\n",
      "<class 'list'>\n",
      "   0 0.3746594605695318\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m tcPINN\u001b[39m.\u001b[39mlayers[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtrainable \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[39m#retrain\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m retrain(tcPINN, \u001b[39m500\u001b[39;49m)\n\u001b[0;32m     29\u001b[0m \u001b[39mfor\u001b[39;00m layer_index \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m9\u001b[39m):\n\u001b[0;32m     31\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m-----------------\u001b[39m\u001b[39m'\u001b[39m, layer_index, \u001b[39m'\u001b[39m\u001b[39m-----------------\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 46\u001b[0m, in \u001b[0;36mretrain\u001b[1;34m(model, epochs)\u001b[0m\n\u001b[0;32m     44\u001b[0m     loss \u001b[39m=\u001b[39m L1 \u001b[39m+\u001b[39m L2\n\u001b[0;32m     45\u001b[0m \u001b[39m# compute gradients\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m g \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39;49mgradient(loss, model\u001b[39m.\u001b[39;49mtrainable_weights)\n\u001b[0;32m     47\u001b[0m loss_list\u001b[39m.\u001b[39mappend(loss)\n\u001b[0;32m     48\u001b[0m \u001b[39m# log every 10 epochs\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pcdcm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:1063\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1057\u001b[0m   output_gradients \u001b[39m=\u001b[39m (\n\u001b[0;32m   1058\u001b[0m       composite_tensor_gradient\u001b[39m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[0;32m   1059\u001b[0m           output_gradients))\n\u001b[0;32m   1060\u001b[0m   output_gradients \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m x \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m ops\u001b[39m.\u001b[39mconvert_to_tensor(x)\n\u001b[0;32m   1061\u001b[0m                       \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m output_gradients]\n\u001b[1;32m-> 1063\u001b[0m flat_grad \u001b[39m=\u001b[39m imperative_grad\u001b[39m.\u001b[39;49mimperative_grad(\n\u001b[0;32m   1064\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tape,\n\u001b[0;32m   1065\u001b[0m     flat_targets,\n\u001b[0;32m   1066\u001b[0m     flat_sources,\n\u001b[0;32m   1067\u001b[0m     output_gradients\u001b[39m=\u001b[39;49moutput_gradients,\n\u001b[0;32m   1068\u001b[0m     sources_raw\u001b[39m=\u001b[39;49mflat_sources_raw,\n\u001b[0;32m   1069\u001b[0m     unconnected_gradients\u001b[39m=\u001b[39;49munconnected_gradients)\n\u001b[0;32m   1071\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_persistent:\n\u001b[0;32m   1072\u001b[0m   \u001b[39m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[0;32m   1073\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_watched_variables \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tape\u001b[39m.\u001b[39mwatched_variables()\n",
      "File \u001b[1;32mc:\\Users\\pcdcm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     65\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mUnknown value for unconnected_gradients: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m unconnected_gradients)\n\u001b[1;32m---> 67\u001b[0m \u001b[39mreturn\u001b[39;00m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_TapeGradient(\n\u001b[0;32m     68\u001b[0m     tape\u001b[39m.\u001b[39;49m_tape,  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m     69\u001b[0m     target,\n\u001b[0;32m     70\u001b[0m     sources,\n\u001b[0;32m     71\u001b[0m     output_gradients,\n\u001b[0;32m     72\u001b[0m     sources_raw,\n\u001b[0;32m     73\u001b[0m     compat\u001b[39m.\u001b[39;49mas_str(unconnected_gradients\u001b[39m.\u001b[39;49mvalue))\n",
      "File \u001b[1;32mc:\\Users\\pcdcm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1085\u001b[0m, in \u001b[0;36m_TapeGradientFunctions._wrap_backward_function.<locals>._backward_function_wrapper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   1083\u001b[0m   \u001b[39mif\u001b[39;00m input_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m backward_function_inputs:\n\u001b[0;32m   1084\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1085\u001b[0m \u001b[39mreturn\u001b[39;00m backward\u001b[39m.\u001b[39;49m_call_flat(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m   1086\u001b[0m     processed_args, remapped_captures)\n",
      "File \u001b[1;32mc:\\Users\\pcdcm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\pcdcm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\pcdcm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#iterative TT decomp + retraining\n",
    "\n",
    "#reset tcPINN\n",
    "og_model = tf.keras.models.load_model(\"burgers.h5\") #loss = 0.00591\n",
    "og_weights = og_model.get_weights()\n",
    "tcPINN.set_weights(og_weights)\n",
    "for i in range(11):\n",
    "    tcPINN.layers[i].set_trainable = True\n",
    "\n",
    "print('----------------- input layer -----------------')\n",
    "\n",
    "#get specific layer of weights\n",
    "layer = tcPINN.get_weights()[0]\n",
    "weights = tcPINN.get_weights()\n",
    "\n",
    "#compress layer\n",
    "compressed_layer = decomp_recomp(layer, 0.75, [5, 4, 2], 'input layer')\n",
    "\n",
    "#input compressed layer\n",
    "weights[0] = compressed_layer\n",
    "tcPINN.set_weights(weights)\n",
    "\n",
    "#freeze layer\n",
    "tcPINN.layers[0].trainable = False\n",
    "\n",
    "#retrain\n",
    "retrain(tcPINN, 500)\n",
    "\n",
    "for layer_index in range(1, 9):\n",
    "\n",
    "    print('-----------------', layer_index, '-----------------')\n",
    "\n",
    "    #get specific layer of weights\n",
    "    layer = tcPINN.get_weights()[layer_index * 2]\n",
    "    weights = tcPINN.get_weights()\n",
    "\n",
    "    #compress layer\n",
    "    compressed_layer = decomp_recomp(layer, 0.75, [10, 8, 5])\n",
    "\n",
    "    #input compressed layer\n",
    "    weights[layer_index * 2] = compressed_layer\n",
    "    tcPINN.set_weights(weights)\n",
    "\n",
    "    #freeze layer\n",
    "    tcPINN.layers[layer_index].trainable = False\n",
    "\n",
    "    #retrain\n",
    "    retrain(tcPINN, 500)\n",
    "\n",
    "    print(tcPINN.get_weights()[layer_index * 2].shape)\n",
    "\n",
    "    \n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
